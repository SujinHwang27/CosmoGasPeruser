{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22214cb2-fa7a-4f8f-b5a6-cc67e83e3b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "pd.set_option(\"display.width\", 1000)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3383e32b-9478-4f7f-bb7f-540fdfcf2cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load 16000 Data * 16 boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dba5c17-086c-4de2-8628-891ef3a92735",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/home/sujin/CosmoGasPeruser/data/Spectra_for_Sujin\"\n",
    "physics_values = [1, 2, 3, 4]\n",
    "redshift_values = [0.1, 0.3, 2.2, 2.4]\n",
    "\n",
    "# Initialize the dictionary to store data by redshift and physics values\n",
    "data_by_redshift = {str(redshift): {str(physics): [] for physics in physics_values} for redshift in redshift_values}\n",
    "# data_by_redshift = {\n",
    "# 0.1 : {1: [[spectrum1],[spectrum2]...], 2:[[]], 3:[[]], 4:[[]]},\n",
    "# 0.3 : {1: [[]], 2:[[]], 3:[[]], 4:[[]]},\n",
    "# 2.2 : {1: [[]], 2:[[]], 3:[[]], 4:[[]]},\n",
    "# 2.4 : {1: [[]], 2:[[]], 3:[[]], 4:[[]]},\n",
    "# }\n",
    "\n",
    "# Iterate through each folder\n",
    "for redshift in redshift_values:\n",
    "    redshift = str(redshift)\n",
    "    for physics in physics_values:\n",
    "        physics = str(physics)\n",
    "        folder_name = f\"{physics}_{redshift}\"\n",
    "        folder = os.path.join(base_dir, folder_name)\n",
    "        print(f\"Processing folder: {folder_name}\")\n",
    "\n",
    "        try:\n",
    "            # Load flux data\n",
    "            flux = np.load(os.path.join(folder, \"flux.npy\"))\n",
    "            data_by_redshift[redshift][physics] = flux  # Store flux data under appropriate redshift and physics\n",
    "\n",
    "            print(f\"Flux data shape for redshift {redshift}, physics {physics}: {flux.shape}\")\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Files not found in folder {folder}: {e}\")\n",
    "            del data_by_redshift[redshift][physics]\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing folder {folder}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Now data_by_redshift contains flux data for each redshift and physics combination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ae59be-6308-40bc-9abd-19a134eff733",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Example of a data point: spectrum number 100 of physics no.4 in redshift=0.3 \\n{data_by_redshift['0.3']['4'][100]}')\n",
    "print(f'Shape: {data_by_redshift['0.3']['4'][100].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fa9eef-25cc-4056-a2cb-817af8f5bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "flux = data_by_redshift['2.4']['4'][100]  \n",
    "wavelength = np.arange(len(flux)) \n",
    "\n",
    "# Plot the spectrum\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.plot(wavelength, flux, label='Spectrum (Physics 4, Redshift 0.3)', color='black', lw=1.5)\n",
    "\n",
    "plt.xlabel('Wavelength index', fontsize=12)  # Use 'Wavelength' if real wavelength data is available\n",
    "plt.ylabel('Flux', fontsize=12)\n",
    "plt.title('Spectrum Plot: Physics 4 at Redshift 0.3', fontsize=14)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6156e503-dec3-4dcb-a07c-919b9d9746ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. ERD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6307a210-1a05-4358-85dc-504df504c980",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" DATA SHAPE AND SIZE AS WELL AS GLOBAL MIN/MAX VALUE \"\"\"\n",
    "# Compute the global minimum and maximum values across all spectra in data_by_redshift\n",
    "all_flux_values = np.concatenate([np.concatenate(list(physics_dict.values())) \n",
    "                                  for physics_dict in data_by_redshift.values()])\n",
    "print(f\"Total {all_flux_values.shape[0]} spectra, of {all_flux_values.shape[1]} features\")\n",
    "\n",
    "MIN_FLUX = np.min(all_flux_values)\n",
    "MAX_FLUX = np.max(all_flux_values)\n",
    "print(f\"Global min flux value, max flux value: {MIN_FLUX}, {MAX_FLUX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a55524b-137c-4968-bbd8-c28c4136c544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize each spectrum in the range [0, 255]\n",
    "def normalize_spectrum(spectrum):\n",
    "    normalized = 255 * (spectrum - MIN_FLUX) / (MAX_FLUX - MIN_FLUX)\n",
    "    return normalized.astype(np.uint8)\n",
    "sample_spectra = [normalize_spectrum(data_by_redshift['2.4']['4'][100])]\n",
    "\n",
    "# Plot the stacked samples\n",
    "plt.figure(figsize=(20, 0.2))\n",
    "plt.imshow(sample_spectra, cmap=\"gray\", aspect=\"auto\", interpolation='none')\n",
    "plt.axis(\"off\")  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b10b25-3118-4fd9-a407-0828c864f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" GRAYSCALE REPRESENTATION \"\"\"\n",
    "def into_grayscale(redshift, data_dict, num_samples):\n",
    "    for physics, spectra in data_dict.items():\n",
    "        if spectra.size > 0:  # Check if spectra is not empty\n",
    "            indices = random.sample(range(len(spectra)), min(num_samples, len(spectra)))  # Sample indices\n",
    "            sample_spectra = [normalize_spectrum(spectra[idx]) for idx in indices]\n",
    "            stacked_samples = np.vstack(sample_spectra)\n",
    "            \n",
    "            # Plot the stacked samples\n",
    "            plt.figure(figsize=(20, 4))\n",
    "            plt.imshow(stacked_samples, cmap=\"gray\", aspect=\"auto\", interpolation='none')\n",
    "            plt.axis(\"off\")  # Hide axes\n",
    "            plt.title(f\"Redshift {redshift}, Physics {physics} : 20 Arrays Concatenated Vertically\")\n",
    "            plt.show()\n",
    "\n",
    "# Sample and visualize 20 spectra per redshift and physics class\n",
    "num_samples = 20\n",
    "for redshift, physics_dict in data_by_redshift.items():\n",
    "    print(\"-\" * 60 + f\"IN REDSHIFT {redshift}\" + \"-\"*60)\n",
    "    into_grayscale(redshift, physics_dict, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909668f3-0743-42de-a004-cdc81d924820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" LOCAL MINIMA IDENTIFICATION PER EACH CLASS \"\"\"\n",
    "\n",
    "def local_minima(flux):\n",
    "    # Find indices of local minima\n",
    "    local_minima_indices = np.where((flux[1:-1] < flux[:-2]) & (flux[1:-1] < flux[2:]))[0] + 1\n",
    "    local_minima_values = flux[local_minima_indices]\n",
    "    return local_minima_indices, local_minima_values\n",
    "\n",
    "# Example of local minima identification of a spectrum \n",
    "# Example array (flux values)\n",
    "sample_spectrum = data_by_redshift[\"0.3\"][\"4\"][10000]  \n",
    "sample_local_minima_indices, sample_local_minima_values = local_minima(sample_spectrum)\n",
    "\n",
    "# print example local minima \n",
    "print(sample_local_minima_indices)\n",
    "print(sample_local_minima_values)\n",
    "print(f\"This spectrum has {len(sample_local_minima_values)} local minimas\")\n",
    "\n",
    "# Plot local minima\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(sample_spectrum, label=\"Flux\", color=\"blue\", linewidth=1)\n",
    "plt.scatter(sample_local_minima_indices, sample_local_minima_values, color=\"red\", label=\"Local Minima\", zorder=5)\n",
    "plt.title(\"Flux Array with Local Minima\")\n",
    "plt.xlabel(\"Wavelength Index\")\n",
    "plt.ylabel(\"Flux Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bee959-f116-41fd-8aa7-19ec26dac1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" DISTRIBUTION OF LOCAL MINIMA COUNTS \"\"\"\n",
    "\n",
    "\"\"\" INDIVIDUAL BAR GRAPH FOR EACH REDSHIFT,PHYSICS \"\"\"\n",
    "def local_minima_occurances(spectra, redshift, physics):\n",
    "    local_minima_counts = []\n",
    "    \n",
    "    for spectrum in spectra:\n",
    "        indices, values = local_minima(spectrum)\n",
    "\n",
    "        local_minima_counts.append(len(indices))    # the number of local minima that this spectrum has\n",
    "\n",
    "    \n",
    "    # Analyze the distribution of the counts\n",
    "    minima_count_mean = np.mean(local_minima_counts)\n",
    "    minima_count_std = np.std(local_minima_counts)\n",
    "    minima_count_median = np.median(local_minima_counts)\n",
    "    minima_count_min = np.min(local_minima_counts)\n",
    "    minima_count_max = np.max(local_minima_counts)\n",
    "    \n",
    "    print(f\"Mean count of local minima: {minima_count_mean:.2f}\")\n",
    "    print(f\"Standard deviation of local minima counts: {minima_count_std:.2f}\")\n",
    "    print(f\"Median count of local minima: {minima_count_median:.2f}\")\n",
    "    print(f\"Min count of local minima: {minima_count_min}\")\n",
    "    print(f\"Max count of local minima: {minima_count_max}\")\n",
    "    \n",
    "    # Visualize the histogram of local minima counts\n",
    "    plt.figure(figsize=((minima_count_max-minima_count_min)/20, 3))\n",
    "    plt.hist(local_minima_counts, bins=minima_count_max-minima_count_min, color='skyblue', edgecolor='black')\n",
    "    plt.title(f\"Redshift {redshift}, Physics {physics}\")\n",
    "    plt.xlabel(\"Number of Local Minima\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return minima_count_mean, minima_count_std, minima_count_median, minima_count_min, minima_count_max\n",
    "\n",
    "results = {str(redshift):pd.DataFrame() for redshift, _ in data_by_redshift.items()}\n",
    "for redshift, physics_dict in data_by_redshift.items():\n",
    "    print(\"\\n\" + \"-\"*40 + f\"local minima count distribution for redshift {redshift}\" + \"-\"*40)\n",
    "    # Loop through each physics category and process data\n",
    "    curr_result = []\n",
    "    for physics, data in physics_dict.items():\n",
    "        # Get the statistics\n",
    "        mean_count, std_dev, median_count, min_count, max_count= local_minima_occurances(data, redshift, physics)\n",
    "        \n",
    "        curr_result.append({\n",
    "            \"Physics\": physics,\n",
    "            \"Mean Count\": mean_count,\n",
    "            \"Standard Deviation\": std_dev,\n",
    "            \"Median Count\": median_count,\n",
    "            \"Min Count\": min_count,\n",
    "            \"Max Count\": max_count,\n",
    "        })\n",
    "    results[redshift] = pd.DataFrame(curr_result)\n",
    "    print(results[redshift])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd886cb-7f6c-436d-9fd0-c86cfa979adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Local Minima Distribution \"\"\"\n",
    "\n",
    "\"\"\" Since the bar plot above is too long and the length varies, cut the graph into half \"\"\"\n",
    "def local_minima_occurances_below_75(spectra, redshift, physics):\n",
    "    local_minima_counts = []\n",
    "    \n",
    "    for spectrum in spectra:\n",
    "        indices, values = local_minima(spectrum)\n",
    "        if len(indices) <= 75:\n",
    "            local_minima_counts.append(len(indices))  # the number of local minima this spectrum has\n",
    "    \n",
    "    if len(local_minima_counts) == 0:\n",
    "        return 0, 0, 0, 0, 0, [], 0\n",
    "    \n",
    "    minima_count_mean = np.mean(local_minima_counts)\n",
    "    minima_count_std = np.std(local_minima_counts)\n",
    "    minima_count_median = np.median(local_minima_counts)\n",
    "    minima_count_min = np.min(local_minima_counts)\n",
    "    minima_count_max = np.max(local_minima_counts)\n",
    "    \n",
    "    unique_counts, counts_frequency = np.unique(local_minima_counts, return_counts=True)\n",
    "    \n",
    "    return minima_count_mean, minima_count_std, minima_count_median, minima_count_min, minima_count_max, (unique_counts, counts_frequency), len(local_minima_counts)\n",
    "\n",
    "\n",
    "results = {str(redshift): pd.DataFrame() for redshift, _ in data_by_redshift.items()}\n",
    "\n",
    "for redshift, physics_dict in data_by_redshift.items():\n",
    "    print(\"\\n\" + \"-\" * 40 + f\" Local minima count distribution for redshift {redshift} \" + \"-\" * 40)\n",
    "    curr_result = []\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(f\"Redshift {redshift}: Distribution of Local Minima Counts Below 75\")\n",
    "    plt.xlabel(\"Number of Local Minima\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    for physics, data in physics_dict.items():\n",
    "        mean_count, std_dev, median_count, min_count, max_count, freq_data, size = local_minima_occurances_below_75(\n",
    "            data, redshift, physics\n",
    "        )\n",
    "        \n",
    "        curr_result.append({\n",
    "            \"Physics\": physics,\n",
    "            \"Mean Count\": mean_count,\n",
    "            \"Standard Deviation\": std_dev,\n",
    "            \"Median Count\": median_count,\n",
    "            \"Min Count\": min_count,\n",
    "            \"Max Count\": max_count,\n",
    "            \"Size\": size\n",
    "        })\n",
    "        \n",
    "        if len(freq_data) > 0:\n",
    "            unique_counts, counts_frequency = freq_data\n",
    "            plt.plot(unique_counts, counts_frequency, label=f\"Physics: {physics}\", marker='o')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    results[redshift] = pd.DataFrame(curr_result)\n",
    "    print(results[redshift])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc7ac93-08e0-4af1-ba9e-5187a75151a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_minima_occurances_above_75(spectra, redshift, physics):\n",
    "    local_minima_counts = []\n",
    "    \n",
    "    for spectrum in spectra:\n",
    "        indices, values = local_minima(spectrum)\n",
    "        if len(indices) > 75:\n",
    "            local_minima_counts.append(len(indices)) \n",
    "    \n",
    "    if len(local_minima_counts) == 0:\n",
    "        return 0, 0, 0, 0, 0, ([], []), 0\n",
    "    \n",
    "    minima_count_mean = np.mean(local_minima_counts)\n",
    "    minima_count_std = np.std(local_minima_counts)\n",
    "    minima_count_median = np.median(local_minima_counts)\n",
    "    minima_count_min = np.min(local_minima_counts)\n",
    "    minima_count_max = np.max(local_minima_counts)\n",
    "\n",
    "\n",
    "    unique_counts, counts_frequency = np.unique(local_minima_counts, return_counts=True)\n",
    "\n",
    "    return minima_count_mean, minima_count_std, minima_count_median, minima_count_min, minima_count_max, (unique_counts, counts_frequency), len(local_minima_counts)\n",
    "\n",
    "\n",
    "results = {str(redshift): pd.DataFrame() for redshift, _ in data_by_redshift.items()}\n",
    "\n",
    "for redshift, physics_dict in data_by_redshift.items():\n",
    "    print(\"\\n\" + \"-\" * 40 + f\" Local minima count distribution for redshift {redshift} \" + \"-\" * 40)\n",
    "    curr_result = []\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(f\"Redshift {redshift}: Local Minima Counts Above 75\")\n",
    "    plt.xlabel(\"Number of Local Minima\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    for physics, data in physics_dict.items():\n",
    "        mean_count, std_dev, median_count, min_count, max_count, freq_data, size = local_minima_occurances_above_75(\n",
    "            data, redshift, physics\n",
    "        )\n",
    "        \n",
    "        curr_result.append({\n",
    "            \"Physics\": physics,\n",
    "            \"Mean Count\": mean_count,\n",
    "            \"Standard Deviation\": std_dev,\n",
    "            \"Median Count\": median_count,\n",
    "            \"Min Count\": min_count,\n",
    "            \"Max Count\": max_count,\n",
    "            \"Size\": size\n",
    "        })\n",
    "        \n",
    "        if len(freq_data[0]) > 0:  \n",
    "            unique_counts, counts_frequency = freq_data\n",
    "            plt.plot(unique_counts, counts_frequency, label=f\"Physics: {physics}\", marker='o')\n",
    "        \n",
    "    results[redshift] = pd.DataFrame(curr_result)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlim(75, max(curr_result, key=lambda x: x[\"Max Count\"])[\"Max Count\"] + 5)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(results[redshift])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2791173f-a930-40ce-b2c5-d4e6514c053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Local Minima Occurrence Index Distribution \"\"\"\n",
    "## At which indices do local minima happen mostly?\n",
    "\n",
    "def get_all_minima_indices(spectra):\n",
    "    all_minima_indices = []\n",
    "    \n",
    "    for flux in spectra:\n",
    "        indices, values = local_minima(flux)\n",
    "        all_minima_indices.extend(indices)\n",
    "    return all_minima_indices\n",
    "    \n",
    "def get_top_minima_indices(all_minima_indices):\n",
    "    index_frequencies = Counter(all_minima_indices)\n",
    "    sorted_indices = sorted(index_frequencies.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    top_indices = sorted_indices[:10]  \n",
    "    print(\"Top 10 indices with most local minima:\")\n",
    "    for index, freq in top_indices:\n",
    "        print(f\"Index {index}: {freq} occurrences\")\n",
    "    return top_indices\n",
    "    \n",
    "def local_minima_counter_histogram(all_minima_indices):\n",
    "    index_frequencies = Counter(all_minima_indices)\n",
    "    indices, frequencies = zip(*index_frequencies.items())\n",
    "    \n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.hist(indices, bins=int(len(indices) / 10), weights=frequencies, color='skyblue', edgecolor='black')\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.title(f\"Redshift {redshift}, Physics {physics} Histogram: Frequency of Local Minima Across Indices\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def local_minima_counter_colorbar(all_minima_indices):\n",
    "    index_frequencies = Counter(all_minima_indices)\n",
    "    indices = np.arange(2048)  \n",
    "    frequencies = np.zeros(2048)  \n",
    "    for index, freq in index_frequencies.items():\n",
    "        frequencies[index] = freq\n",
    "    norm = plt.Normalize(vmin=frequencies.min(), vmax=frequencies.max())\n",
    "    \n",
    "    plt.figure(figsize=(20, 2)) \n",
    "    plt.scatter(indices, np.zeros_like(indices), c=frequencies, cmap='viridis', norm=norm, s=10)\n",
    "    \n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label(\"Frequency of Local Minima\", rotation=270, labelpad=15)\n",
    "    \n",
    "    plt.title(f\"Redshift {redshift}, Physics {physics} Colorbar: Frequency of Local Minima Across Indices\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.yticks([])  \n",
    "    plt.gca().axes.get_yaxis().set_visible(False)  \n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.5)  \n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "results = {str(redshift):pd.DataFrame() for redshift, _ in data_by_redshift.items()}\n",
    "for redshift, physics_dict in data_by_redshift.items():\n",
    "    curr_result = []\n",
    "    for physics, data in physics_dict.items():\n",
    "        print(\"-\"*60 + f\"Redshift {redshift}, Physics {physics}\", \"-\"*60)\n",
    "        minima_indices = get_all_minima_indices(data)\n",
    "        local_minima_counter_histogram(minima_indices)\n",
    "        local_minima_counter_colorbar(minima_indices)\n",
    "        curr_result.append([index for index,_ in get_top_minima_indices(minima_indices)])\n",
    "    print(f\"Redshift {redshift}: Top 10 popular wavelengh indices with the most local minima occurances\")\n",
    "    results[redshift] = pd.DataFrame(curr_result)\n",
    "    results[redshift].columns = [i for i in range(1, 11)]\n",
    "    results[redshift].index = [f\"Physics{i}\" for i in range(1, len(data_by_redshift[redshift])+1)]\n",
    "    print(results[redshift])\n",
    "\n",
    "## Conclusion: they happen everywhere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b61fde7-bf6d-477e-99b8-3ce6d833a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" FURTHER EXPLORATION: IDENTIFY LOCAL MINIMA CENTER, WIDTH, DEPTH \"\"\"\n",
    "\"\"\" The method to fit the wells needs to be advised \"\"\"\n",
    "# Example flux array\n",
    "flux = data_by_redshift[\"0.3\"][\"4\"][10000][:500]\n",
    "\n",
    "# Find local minima indices\n",
    "local_minima_indices = np.where((flux[1:-1] < flux[:-2]) & (flux[1:-1] < flux[2:]))[0] + 1\n",
    "\n",
    "# Initialize widths list\n",
    "minima_widths = []\n",
    "\n",
    "def calculate_minimum_width(flux, minima_index):\n",
    "    min_value = flux[minima_index]  # Flux at the minimum\n",
    "    left_index = minima_index - 1\n",
    "    right_index = minima_index + 1\n",
    "   \n",
    "    while left_index > 0 and flux[left_index-1] < flux[left_index]:\n",
    "        left_index -= 1\n",
    "\n",
    "    while right_index < len(flux) - 1 and flux[right_index] < flux[right_index+1]:\n",
    "        right_index += 1\n",
    "\n",
    "    width = right_index - left_index\n",
    "    return width\n",
    "\n",
    "for index in local_minima_indices:\n",
    "    width = calculate_minimum_width(flux, index)\n",
    "    minima_widths.append(width)\n",
    "\n",
    "print(local_minima_indices)\n",
    "print(minima_widths)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(flux, label=\"Flux\", color=\"blue\", linewidth=1)\n",
    "plt.scatter(local_minima_indices, flux[local_minima_indices], color=\"red\", label=\"Local Minima\", zorder=3)\n",
    "\n",
    "for i, index in enumerate(local_minima_indices):\n",
    "    plt.text(index, flux[index], f\"{minima_widths[i]:.0f}\", color=\"green\", fontsize=8)\n",
    "\n",
    "plt.title(\"Flux Array with Local Minima and Widths\")\n",
    "plt.xlabel(\"Wavelength Index\")\n",
    "plt.ylabel(\"Flux Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "for i, index in enumerate(local_minima_indices):\n",
    "    print(f\"Local Minimum at Index {index}, Depth: {flux[index]:.4f}, Width: {minima_widths[i]}\")\n",
    "\n",
    "# Not really clear how subtle the absorption well should be to be disregarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e65c253-25a3-462f-8584-670ad5d0d931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db32a64-0530-422a-9002-45eabe42b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-a. Original dataset by redshift: spectra = {'0.1': [[]], '0.3': [[]], ... }\n",
    "spectra = {str(redshift): [] for redshift in redshift_values}\n",
    "spectra_labels = {str(redshift): [] for redshift in redshift_values}\n",
    "\n",
    "# Iterate over each redshift\n",
    "for redshift, physics_dict in data_by_redshift.items():\n",
    "    # Iterate over each physics (class)\n",
    "    for physics, fluxes in physics_dict.items():\n",
    "        for flux in fluxes:  # fluxes is a list of spectra\n",
    "            spectra[redshift].append(flux)\n",
    "            spectra_labels[redshift].append(physics)  # The label is the physics value\n",
    "    spectra[redshift] = np.array(spectra[redshift])\n",
    "    spectra_labels[redshift] = np.array(spectra_labels[redshift])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f7e325-efda-4d88-b103-a6827ed4bef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all spectra arrays across redshifts\n",
    "all_spectra = np.concatenate([spectra_by_redshift for spectra_by_redshift in spectra.values()])\n",
    "print(all_spectra.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667450c0-6412-40e1-a2e8-d5c013e75936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced data for each redshifts: \"pca_data_by_redshift\"\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "var_95_n_components = {}\n",
    "var_90_n_components = {}\n",
    "var_85_n_components = {}\n",
    "var_80_n_components = {}\n",
    "var_75_n_components = {}\n",
    "var_70_n_components = {}\n",
    "var_65_n_components = {}\n",
    "var_60_n_components = {}\n",
    "\n",
    "for redshift, curr_spectra in spectra.items():\n",
    "    print(f\"\\nPCA analysis for redshift {redshift}\")\n",
    "    # spectra = np.concatenate([data for data in data_by_redshift[redshift].values()])\n",
    "    print(f\"Total {curr_spectra.shape[0]} spectra, each of {curr_spectra.shape[1]} long\")\n",
    "    pca = PCA().fit(curr_spectra)  # Fit PCA without specifying n_components\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('Explained Variance vs. Number of Components')\n",
    "    %matplotlib inline\n",
    "    plt.figure(figsize=(4, 2.5))\n",
    "    plt.show()\n",
    "    \n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "    n_components_var95 = np.where(cumulative_variance >= 0.95)[0][0] + 1\n",
    "    n_components_var90 = np.where(cumulative_variance >= 0.90)[0][0] + 1\n",
    "    n_components_var85 = np.where(cumulative_variance >= 0.85)[0][0] + 1\n",
    "    n_components_var80 = np.where(cumulative_variance >= 0.80)[0][0] + 1\n",
    "    n_components_var75 = np.where(cumulative_variance >= 0.75)[0][0] + 1\n",
    "    n_components_var70 = np.where(cumulative_variance >= 0.70)[0][0] + 1\n",
    "    n_components_var65 = np.where(cumulative_variance >= 0.65)[0][0] + 1\n",
    "    n_components_var60 = np.where(cumulative_variance >= 0.60)[0][0] + 1\n",
    "\n",
    "    var_95_n_components[redshift] = n_components_var95\n",
    "    var_90_n_components[redshift] = n_components_var90\n",
    "    var_85_n_components[redshift] = n_components_var85\n",
    "    var_80_n_components[redshift] = n_components_var80\n",
    "    var_75_n_components[redshift] = n_components_var75\n",
    "    var_70_n_components[redshift] = n_components_var70\n",
    "    var_65_n_components[redshift] = n_components_var65\n",
    "    var_60_n_components[redshift] = n_components_var60\n",
    "\n",
    "    print(f\"Number of components to retain 95% variance for redshift {redshift} : {n_components_var95}\")\n",
    "    print(f\"Number of components to retain 90% variance for redshift {redshift} : {n_components_var90}\")\n",
    "    print(f\"Number of components to retain 85% variance for redshift {redshift} : {n_components_var85}\")\n",
    "    print(f\"Number of components to retain 80% variance for redshift {redshift} : {n_components_var80}\")\n",
    "    print(f\"Number of components to retain 75% variance for redshift {redshift} : {n_components_var75}\")\n",
    "    print(f\"Number of components to retain 70% variance for redshift {redshift} : {n_components_var70}\")\n",
    "    print(f\"Number of components to retain 65% variance for redshift {redshift} : {n_components_var65}\")\n",
    "    print(f\"Number of components to retain 60% variance for redshift {redshift} : {n_components_var60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc029cf-c3e1-4bf6-8a0f-49403be5cd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_transform(dict_n_components):    # Use specific n_components values for each redshift\n",
    "    reduced_data_by_redshift = {str(redshift): {str(physics): [] for physics in [1,2,3,4]} for redshift in data_by_redshift.keys()}\n",
    "    for redshift, physics_dict in data_by_redshift.items():\n",
    "        n_components = dict_n_components[redshift]  # Get n_components for this redshift\n",
    "        print(f\"\\nApplying PCA on redshift {redshift} with n_components = {n_components}\")\n",
    "        for physics, data in physics_dict.items():\n",
    "            pca = PCA(n_components=n_components)\n",
    "            pca_transformed = pca.fit_transform(data)\n",
    "            \n",
    "            reduced_data_by_redshift[redshift][physics] = pca_transformed\n",
    "            \n",
    "            print(f\"Transformed shape for physics {physics}: {reduced_data_by_redshift[redshift][physics].shape}\")\n",
    "    \n",
    "    del reduced_data_by_redshift['0.1']['4']     \n",
    "    \n",
    "    return reduced_data_by_redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d8b050-e03c-4ff9-afeb-d4ee0b58901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reducing data with 95% variance\")\n",
    "pca_95_data_by_redshift = pca_transform(var_95_n_components)   # var_95_n_components = {'0.1':111, '0.3':98, '2.2':106, '2.4':109}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b20aa0c-a0f9-44ea-be80-2d0db739cf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reducing data with 90% variance\")\n",
    "pca_90_data_by_redshift = pca_transform(var_90_n_components)    # var_90_n_components = {'0.1':84, '0.3':73, '2.2':81, '2.4':83}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407ffeb3-8f22-4063-b816-e0e33131a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reducing data with 85% variance\")\n",
    "pca_85_data_by_redshift = pca_transform(var_85_n_components)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a0f88c-8857-4933-89b5-899f47ac1ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reducing data with 80% variance\")\n",
    "pca_80_data_by_redshift = pca_transform(var_80_n_components)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faee5a3-fd03-478d-aeec-ec1f176302ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reducing data with 75% variance\")\n",
    "pca_75_data_by_redshift = pca_transform(var_75_n_components)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c65c6a6-4d07-44f2-8f74-4bb407454bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reducing data with 70% variance\")\n",
    "pca_70_data_by_redshift = pca_transform(var_70_n_components)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b575e24-dc52-48e2-921d-cc76020d295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reducing data with 65% variance\")\n",
    "pca_65_data_by_redshift = pca_transform(var_65_n_components)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f57d9c4-b5c0-4dba-aec1-8db86b092263",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Reducing data with 60% variance\")\n",
    "pca_60_data_by_redshift = pca_transform(var_60_n_components)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b6db0b-8740-460f-b3b2-a13cc296209d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\" How PCA transformation looks like for spectrum data \"\"\"\n",
    "num_samples = 3  # Number of spectra to visualize per redshift and physics\n",
    "\n",
    "for redshift, physics_dict in data_by_redshift.items():\n",
    "    for physics, original_spectra in physics_dict.items():\n",
    "        \n",
    "        sample_indices = np.random.choice(len(original_spectra), num_samples, replace=False)\n",
    "        original_samples = np.array([original_spectra[idx] for idx in sample_indices])\n",
    "        \n",
    "        pca_90_samples = pca_90_data_by_redshift[redshift][physics][sample_indices]\n",
    "        pca_95_samples = pca_95_data_by_redshift[redshift][physics][sample_indices]\n",
    "        \n",
    "        pca_90 = PCA(n_components=var_90_n_components[redshift])\n",
    "        pca_90.fit(original_spectra)  \n",
    "        reconstructed_90_samples = pca_90.inverse_transform(pca_90_samples)\n",
    "        \n",
    "        pca_95 = PCA(n_components=var_95_n_components[redshift])\n",
    "        pca_95.fit(original_spectra)  \n",
    "        reconstructed_95_samples = pca_95.inverse_transform(pca_95_samples)\n",
    "\n",
    "        fig, axes = plt.subplots(num_samples, 3, figsize=(15, num_samples * 2))\n",
    "        fig.suptitle(f\"Redshift {redshift}, Physics {physics} - Original vs PCA-90% vs PCA-95% Reconstructed Spectra\", fontsize=14)\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            mse_90 = mean_squared_error(original_samples[i], reconstructed_90_samples[i])\n",
    "            mse_95 = mean_squared_error(original_samples[i], reconstructed_95_samples[i])\n",
    "            cos_sim_90 = cosine_similarity(original_samples[i].reshape(1, -1), reconstructed_90_samples[i].reshape(1, -1))[0, 0]\n",
    "            cos_sim_95 = cosine_similarity(original_samples[i].reshape(1, -1), reconstructed_95_samples[i].reshape(1, -1))[0, 0]\n",
    "            \n",
    "            axes[i, 0].plot(original_samples[i], color='orange')\n",
    "            axes[i, 0].set_title(\"Original\")\n",
    "            \n",
    "            axes[i, 1].plot(reconstructed_90_samples[i], color='green')\n",
    "            axes[i, 1].set_title(\"PCA-90% Reconstructed\")\n",
    "            axes[i, 1].text(0.15, -0.0, f\"MSE: {mse_90:.4f}\\nCosine Sim: {cos_sim_90:.4f}\", transform=axes[i, 1].transAxes,\n",
    "                            fontsize=8, ha='center', va='bottom', \n",
    "                            bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.3))\n",
    "            \n",
    "            axes[i, 2].plot(reconstructed_95_samples[i], color='blue')\n",
    "            axes[i, 2].set_title(\"PCA-95% Reconstructed\")\n",
    "            axes[i, 2].text(0.15, -0.0, f\"MSE: {mse_95:.4f}\\nCosine Sim: {cos_sim_95:.4f}\", transform=axes[i, 2].transAxes,\n",
    "                            fontsize=8, ha='center', va='bottom', \n",
    "                            bbox=dict(boxstyle=\"round,pad=0.3\", edgecolor=\"green\", facecolor=\"lightgreen\", alpha=0.3))\n",
    "        \n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])  \n",
    "        plt.show()\n",
    "\n",
    "        fig, axes = plt.subplots(num_samples, 3, figsize=(15, num_samples * 2))\n",
    "        fig.suptitle(f\"Redshift {redshift}, Physics {physics} - Original vs PCA-90% vs PCA-95% Reduced Spectra\", fontsize=14)\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            axes[i, 0].plot(original_samples[i], color='orange')\n",
    "            axes[i, 0].set_title(\"Original\")\n",
    "            \n",
    "            axes[i, 1].plot(pca_90_samples[i], color='green')\n",
    "            axes[i, 1].set_title(\"PCA-90% Reduced\")\n",
    "            \n",
    "            axes[i, 2].plot(pca_95_samples[i], color='blue')\n",
    "            axes[i, 2].set_title(\"PCA-95% Reduced\")\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])  \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda03c0f-f0a4-480e-96b7-49d7ab5b8839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Train SVM with k-fold cross validation and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070e07ac-241d-40f1-a80e-ab7d41ed37e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dump_gs_results_analysis(gs_results, excel_file):    \n",
    "    gs_df = {}\n",
    "    for redshift, grid_search in gs_results.items():\n",
    "        df = pd.DataFrame.from_dict(grid_search.cv_results_)\n",
    "        \n",
    "        df['overfit'] = df['mean_train_score'] - df['mean_test_score']\n",
    "    \n",
    "        condition_1 = df['mean_train_score'] == 1\n",
    "        condition_2 = abs(df['mean_test_score'] - df['mean_train_score']) == \\\n",
    "                      abs(df['mean_test_score'] - df['mean_train_score']).min()\n",
    "    \n",
    "        df['highlight'] = ''  \n",
    "        df.loc[condition_1, 'highlight'] = 'red'\n",
    "        df.loc[condition_2, 'highlight'] = 'green'\n",
    "    \n",
    "        gs_df[redshift] = df\n",
    "    \n",
    "    with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n",
    "        for redshift, df in gs_df.items():\n",
    "            df.to_excel(writer, sheet_name=f'redshift_{redshift}', index=False)\n",
    "    \n",
    "    wb = load_workbook(excel_file)\n",
    "    \n",
    "    for redshift, df in gs_df.items():\n",
    "        sheet = wb[f'redshift_{redshift}']\n",
    "        \n",
    "        for row_idx, highlight in enumerate(df['highlight'], start=2):  \n",
    "            fill = None\n",
    "            if highlight == 'red':\n",
    "                fill = PatternFill(start_color=\"E6B8B7\", end_color=\"E6B8B7\", fill_type=\"solid\")\n",
    "            elif highlight == 'green':\n",
    "                fill = PatternFill(start_color=\"B7DEE8\", end_color=\"B7DEE8\", fill_type=\"solid\")\n",
    "            \n",
    "            if fill:\n",
    "                for col_idx in range(1, len(df.columns) + 1):  \n",
    "                    sheet.cell(row=row_idx, column=col_idx).fill = fill\n",
    "    \n",
    "    wb.save(excel_file)\n",
    "\n",
    "\n",
    "def svm_tune_hyperparam(param_grid, size_per_class, pca_ed_data_by_redshift, n_comp):\n",
    "    grid_search_results = {}\n",
    "    \n",
    "    for redshift, physics_dict in pca_ed_data_by_redshift.items():\n",
    "        \n",
    "        print(f\"\\nTraining SVM for redshift {redshift}\")\n",
    "        X = []\n",
    "        y = []\n",
    "        print(f\"redshift: {redshift}\")\n",
    "        for physics, data in physics_dict.items():\n",
    "            indices = np.random.choice(data.shape[0], size=size_per_class, replace=False)\n",
    "            X.append(data[indices])  \n",
    "            y.extend([physics] * size_per_class)  \n",
    "\n",
    "        X = np.vstack(X)\n",
    "        y = np.array(y).astype(int) \n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40)\n",
    "        print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "        \n",
    "        grid_search = GridSearchCV(SVC(), param_grid, cv=KFold(n_splits=5, shuffle=True), scoring='accuracy', verbose=1, return_train_score=True)\n",
    "        \n",
    "        print(\"Performing grid search...\")\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        best_params = grid_search.best_params_\n",
    "\n",
    "        print(f\"Best parameters for redshift {redshift}: {best_params}\")\n",
    "        print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
    "        \n",
    "        grid_search_results[redshift] = grid_search\n",
    "        dump_gs_results_analysis(grid_search_results, f\"gs_results_{n_comp}_{size_per_class}.xlsx\")    \n",
    "        \n",
    "    return grid_search_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1495646-22b8-4d40-8103-1214814d85dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "param_grid_1 = {\n",
    "    'C': [1, 10, 100],  \n",
    "    'gamma': [0.1, 1, 10],  \n",
    "    'kernel': ['rbf', 'linear', 'poly']  \n",
    "}\n",
    "\n",
    "pca_85_300_gs_results = svm_tune_hyperparam(param_grid, 300, pca_85_data_by_redshift, 85)\n",
    "pca_80_300_gs_results = svm_tune_hyperparam(param_grid, 300, pca_80_data_by_redshift, 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817e75c5-74cb-4ada-bcba-245d16bbccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_2 = {\n",
    "    'C': [1, 0.1, 0.01],  \n",
    "    'gamma': [0.1, 1, 10],  \n",
    "    'kernel': ['poly'] \n",
    "}\n",
    "\n",
    "pca_85_500_gs_results = svm_tune_hyperparam(param_grid_2, 500, pca_85_data_by_redshift, 85)\n",
    "pca_80_500_gs_results = svm_tune_hyperparam(param_grid_2, 500, pca_80_data_by_redshift, 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92a8d52-529f-438d-b49b-86611e27e7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_3 = {\n",
    "    'C': [0.1, 0.01, 0.001],  \n",
    "    'gamma': [0.1, 1, 10],  \n",
    "    'kernel': ['poly'] \n",
    "}\n",
    "\n",
    "pca_85_800_gs_results = svm_tune_hyperparam(param_grid_3, 800, pca_85_data_by_redshift, 85)\n",
    "pca_80_800_gs_results = svm_tune_hyperparam(param_grid_3, 800, pca_80_data_by_redshift, 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1159ced-8fb6-4682-8635-c5cfbf8a8ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_4 = {\n",
    "    'C': [0.01, 0.001, 0.0001],  \n",
    "    'gamma': [0.1, 1, 10],  \n",
    "    'kernel': ['poly'] \n",
    "}\n",
    "\n",
    "pca_85_1000_gs_results = svm_tune_hyperparam(param_grid_4, 1000, pca_85_data_by_redshift, 85)\n",
    "pca_80_1000_gs_results = svm_tune_hyperparam(param_grid_4, 1000, pca_80_data_by_redshift, 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e7b054-5da4-4b64-b389-16dc26aa8af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_5 = {\n",
    "    'C': [0.005, 0.001, 0.0005],  \n",
    "    'gamma': [0.1, 1, 10],  \n",
    "    'kernel': ['poly'] \n",
    "}\n",
    "\n",
    "pca_85_500_gs_results = svm_tune_hyperparam(param_grid_5, 2000, pca_85_data_by_redshift, 85)\n",
    "pca_80_500_gs_results = svm_tune_hyperparam(param_grid_5, 2000, pca_80_data_by_redshift, 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87e7b40-d74b-4aba-b624-9d5999c06e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_5 = {\n",
    "    'C': [0.005, 0.001, 0.0005],  \n",
    "    'gamma': [0.1, 1, 10],  \n",
    "    'kernel': ['poly'] \n",
    "}\n",
    "\n",
    "pca_75_2000_gs_results = svm_tune_hyperparam(param_grid_5, 2000, pca_75_data_by_redshift, 75)\n",
    "pca_70_2000_gs_results = svm_tune_hyperparam(param_grid_5, 2000, pca_70_data_by_redshift, 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156e319f-22cb-450e-965c-9e708cf4081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_5 = {\n",
    "    'C': [0.005, 0.001, 0.0005],\n",
    "    'gamma': [0.1, 1, 10],  \n",
    "    'kernel': ['poly'] \n",
    "}\n",
    "\n",
    "pca_65_2000_gs_results = svm_tune_hyperparam(param_grid_5, 2000, pca_65_data_by_redshift, 65)\n",
    "pca_60_2000_gs_results = svm_tune_hyperparam(param_grid_5, 2000, pca_60_data_by_redshift, 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26efe402-24ed-4f97-84a4-044d0343192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_6 = {\n",
    "    'C': [0.005, 0.001, 0.0005], \n",
    "    'gamma': [0.005, 0.01, 0.05],  \n",
    "    'kernel': ['linear', 'poly', 'rbf'] \n",
    "}\n",
    "\n",
    "pca_85_2500_gs_results = svm_tune_hyperparam(param_grid_6, 2500, pca_85_data_by_redshift, 85)\n",
    "pca_80_2500_gs_results = svm_tune_hyperparam(param_grid_6, 2500, pca_80_data_by_redshift, 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c878063f-eee5-4d4c-b37d-9a34680b49ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_75_2500_gs_results = svm_tune_hyperparam(param_grid_6, 2500, pca_75_data_by_redshift, 75)\n",
    "pca_70_2500_gs_results = svm_tune_hyperparam(param_grid_6, 2500, pca_70_data_by_redshift, 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8829c3-226d-4a30-99d1-aa9736eead5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Evaluation and Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a130333-f8b8-4899-9d33-ce9245427c4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_dir = \"./\"\n",
    "output_file = \"consolidated_gs_results.xlsx\"\n",
    "\n",
    "consolidated_data = {\n",
    "    \"redshift_0.1\": [],\n",
    "    \"redshift_0.3\": [],\n",
    "    \"redshift_2.2\": [],\n",
    "    \"redshift_2.4\": []\n",
    "}\n",
    "\n",
    "for file_name in os.listdir(base_dir):\n",
    "    print(f\"Processing {file_name}\")\n",
    "    if file_name.endswith(\".xlsx\") and file_name.startswith(\"gs_results_\"):\n",
    "        file_path = os.path.join(base_dir, file_name)\n",
    "        \n",
    "        n_comp, data_size = file_name.replace(\".xlsx\", \"\").split(\"_\")[2:]\n",
    "        print(f\"n_comp={n_comp}, data_size={data_size}\")\n",
    "        \n",
    "        xls = pd.ExcelFile(file_path)\n",
    "        \n",
    "        for sheet_name in xls.sheet_names:\n",
    "            print(f\"On sheet {sheet_name}\")\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "            \n",
    "            df['n_comp'] = int(n_comp)\n",
    "            df['data_size'] = int(data_size)\n",
    "        \n",
    "            consolidated_data[sheet_name].append(df)\n",
    "\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    for sheet_name, data_list in consolidated_data.items():\n",
    "        combined_df = pd.concat(data_list, ignore_index=True)\n",
    "        combined_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"Consolidation complete! Results saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164c7283-396e-4287-b431-b430210ff461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_vs_overfit():\n",
    "    excel_file = \"consolidated_gs_results.xlsx\"\n",
    "    sheets = pd.ExcelFile(excel_file).sheet_names  \n",
    "    \n",
    "    for sheet in sheets:\n",
    "        df = pd.read_excel(excel_file, sheet_name=sheet)\n",
    "        \n",
    "        if 'overfit' not in df.columns or 'mean_test_score' not in df.columns:\n",
    "            print(f\"Skipping sheet {sheet}: required columns not found.\")\n",
    "            continue\n",
    "        \n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.scatter(df['mean_test_score'], df['overfit'], alpha=0.7, c='blue', label='Overfit vs Test Score')\n",
    "        plt.axhline(0, color='red', linestyle='--', linewidth=1, label='No Overfit Line')\n",
    "        \n",
    "        plt.title(f\"Overfit vs Mean Test Score for {sheet}\")\n",
    "        plt.xlabel(\"Mean Test Score\")\n",
    "        plt.ylabel(\"Overfit (Train - Test Score)\")\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6ce2ca-7f2a-49cc-a0fe-3593658557a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def factors_heatmap():\n",
    "    file_path = 'consolidated_gs_results.xlsx'\n",
    "    sheet_names = ['redshift_0.1', 'redshift_0.3', 'redshift_2.2', 'redshift_2.4']\n",
    "    \n",
    "    columns_to_include = ['param_C', 'param_gamma', 'param_kernel', 'mean_test_score', \n",
    "                          'mean_train_score', 'overfit', 'n_comp', 'data_size']\n",
    "    \n",
    "    for sheet in sheet_names:\n",
    "        data = pd.read_excel(file_path, sheet_name=sheet)\n",
    "        selected_data = data[columns_to_include]\n",
    "        selected_data = pd.get_dummies(selected_data, drop_first=False)  # One-hot encoding\n",
    "        correlation_matrix = selected_data.corr()\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", cbar=True)\n",
    "        plt.title(f\"Factors correlation Heatmap for {sheet}\")\n",
    "        plt.show()\n",
    "\n",
    "factors_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235f7582-5513-4f0f-a7ba-3d4776140d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_7 = {\n",
    "    'C': [0.005, 0.001, 0.01],  \n",
    "    'gamma': [0.01, 0.03, 0.1],  \n",
    "    'kernel': ['poly'] \n",
    "}\n",
    "\n",
    "pca_85_3000_gs_results = svm_tune_hyperparam(param_grid_7, 3000, pca_85_data_by_redshift, 85)\n",
    "pca_80_3000_gs_results = svm_tune_hyperparam(param_grid_7, 3000, pca_80_data_by_redshift, 80)\n",
    "pca_75_3000_gs_results = svm_tune_hyperparam(param_grid_7, 3000, pca_75_data_by_redshift, 75)\n",
    "pca_70_3000_gs_results = svm_tune_hyperparam(param_grid_7, 3000, pca_70_data_by_redshift, 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30692f84-42a5-4011-836a-9c3e6b2a4ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_8 = {\n",
    "    'C': [0.005, 0.001, 0.01],  \n",
    "    'gamma': [0.8, 1, 2],  \n",
    "    'kernel': ['poly'] \n",
    "}\n",
    "\n",
    "pca_85_3300_gs_results = svm_tune_hyperparam(param_grid_8, 3300, pca_85_data_by_redshift, 85)\n",
    "pca_80_3300_gs_results = svm_tune_hyperparam(param_grid_8, 3300, pca_80_data_by_redshift, 80)\n",
    "pca_75_3300_gs_results = svm_tune_hyperparam(param_grid_8, 3300, pca_75_data_by_redshift, 75)\n",
    "pca_70_3300_gs_results = svm_tune_hyperparam(param_grid_8, 3300, pca_70_data_by_redshift, 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc101a3e-b9c7-4c9d-83f5-7161fe84807f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "factors_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3423e343-ee02-415c-8c36-2ff8146c4d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Hyperparameter Tuning again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10abcaa-dd11-44b2-b1e7-b68a42d172d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_9 = {\n",
    "    'C': [0.005, 0.001, 0.01],  \n",
    "    'gamma': [2, 3, 4],  \n",
    "    'kernel': ['poly'] \n",
    "}\n",
    "\n",
    "pca_85_3500_gs_results = svm_tune_hyperparam(param_grid_9, 3500, pca_85_data_by_redshift, 85)\n",
    "pca_80_3500_gs_results = svm_tune_hyperparam(param_grid_9, 3500, pca_80_data_by_redshift, 80)\n",
    "pca_75_3500_gs_results = svm_tune_hyperparam(param_grid_9, 3500, pca_75_data_by_redshift, 75)\n",
    "pca_70_3500_gs_results = svm_tune_hyperparam(param_grid_9, 3500, pca_70_data_by_redshift, 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1469dc-4fd8-4289-ad3f-d42b8dcee2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_10 = {\n",
    "    'C': [0.005, 0.001, 0.01],  \n",
    "    'gamma': [0.01, 0.02, 0.03],  \n",
    "    'kernel': ['poly'] \n",
    "}\n",
    "\n",
    "pca_85_3700_gs_results = svm_tune_hyperparam(param_grid_10, 3700, pca_85_data_by_redshift, 85)\n",
    "pca_80_3700_gs_results = svm_tune_hyperparam(param_grid_10, 3700, pca_80_data_by_redshift, 80)\n",
    "pca_75_3700_gs_results = svm_tune_hyperparam(param_grid_10, 3700, pca_75_data_by_redshift, 75)\n",
    "pca_70_3700_gs_results = svm_tune_hyperparam(param_grid_10, 3700, pca_70_data_by_redshift, 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70ac2e6-7360-4bca-a7fb-38222f1147ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_vs_overfit():\n",
    "    excel_file = \"consolidated_gs_results.xlsx\"\n",
    "    sheets = pd.ExcelFile(excel_file).sheet_names  # Get all sheet names\n",
    "    \n",
    "    for sheet in sheets:\n",
    "        # Load the data for the current sheet\n",
    "        df = pd.read_excel(excel_file, sheet_name=sheet)\n",
    "        \n",
    "        # Ensure the required columns are present\n",
    "        required_columns = ['overfit', 'mean_test_score', 'data_size', 'n_comp']    \n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            print(f\"Skipping sheet {sheet}: required columns not found.\")\n",
    "            continue\n",
    "        \n",
    "        # Compute the ratio for coloring\n",
    "        df['size_to_comp_ratio'] = df['data_size'] / df['n_comp']\n",
    "        \n",
    "        # Plot the graph\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        scatter = plt.scatter(\n",
    "            df['mean_test_score'], \n",
    "            df['overfit'], \n",
    "            c=df['size_to_comp_ratio'], \n",
    "            cmap='viridis', \n",
    "            alpha=0.7, \n",
    "            label='Overfit vs Test Score'\n",
    "        )\n",
    "        plt.axhline(0, color='red', linestyle='--', linewidth=1, label='No Overfit Line')\n",
    "        \n",
    "        # Adding color bar to explain ratio\n",
    "        cbar = plt.colorbar(scatter)\n",
    "        cbar.set_label('Data Size / n_feature Ratio')\n",
    "        \n",
    "        # Adding titles and labels\n",
    "        plt.title(f\"Overfit vs Mean Test Score for {sheet}\")\n",
    "        plt.xlabel(\"Mean Test Score\")\n",
    "        plt.ylabel(\"Overfit (Train - Test Score)\")\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.5)\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "performance_vs_overfit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0b0654-17bf-42ab-865e-e55899c0c30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aef67e-6737-4c75-8ed1-09726421d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Train all over with the best hyperparameter and full dataset\n",
    "# skipping this part due to limited computational resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5311785-437b-461e-b4c5-0b633be39ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_train_with_best_hyperparam(grid_search_results, full_data):\n",
    "    for redshift, grid_search in grid_search_results.items(): \n",
    "        # best_params = grid_search.best_params_\n",
    "        # print(f\"Best parameters for redshift {redshift}: {best_params}\")\n",
    "        # print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
    "        \n",
    "        best_svm = SVC(**best_params)\n",
    "        best_svm.fit(X_train, y_train)\n",
    "    \n",
    "        model_filename = f\"svm_pca_.pkl\"\n",
    "        joblib.dump(best_svm, model_filename)\n",
    "        logging.info(f\"Model for redshift {redshift} saved to {model_filename}\")\n",
    "        \n",
    "        train_accuracy = accuracy_score(y_train, best_svm.predict(X_train))\n",
    "        test_accuracy = accuracy_score(y_test, best_svm.predict(X_test))\n",
    "        print(f\"Train accuracy for redshift {redshift}: {train_accuracy:.4f}\")\n",
    "        print(f\"Test accuracy for redshift {redshift}: {test_accuracy:.4f}\")\n",
    "        grid_search_results[redshift] = grid_search.cv_results_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a145af3f-5c1e-4e00-93b6-ec59b330402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Training SVM with Feature Extraction: 6 features\n",
    "# Somehow this task takes too long even with very small amount of sample data. \n",
    "# Skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9889cd1d-e4b7-4c24-8321-efb5a195d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_local_minima(data_by_redshift, num_samples=1000, sample_size=100):\n",
    "    new_dict = {\n",
    "        redshift: {physics: [] for physics in physics_dict} \n",
    "        for redshift, physics_dict in data_by_redshift.items()\n",
    "    }\n",
    "    for redshift, physics_dict in data_by_redshift.items():\n",
    "        for physics, flux_data in physics_dict.items():            \n",
    "            new_dataset = []  \n",
    "            \n",
    "            all_local_minima = []\n",
    "            for spectrum in flux_data:\n",
    "                indices, _ = local_minima(spectrum)\n",
    "                all_local_minima.append(indices)\n",
    "            \n",
    "            for _ in range(num_samples):\n",
    "                sampled_indices = random.sample(range(len(all_local_minima)), sample_size)\n",
    "                sampled_minima = [all_local_minima[i] for i in sampled_indices]\n",
    "                \n",
    "                concatenated = np.concatenate(sampled_minima)\n",
    "                stats = [\n",
    "                    round(np.mean(concatenated),4),\n",
    "                    round(np.min(concatenated),4),\n",
    "                    round(np.max(concatenated),4),\n",
    "                    round(np.std(concatenated),4),\n",
    "                    round(np.median(concatenated),4),\n",
    "                    len(concatenated)\n",
    "                ]\n",
    "                new_dataset.append(stats)\n",
    "            \n",
    "            new_dict[redshift][physics] = np.array(new_dataset)\n",
    "    \n",
    "    return new_dict\n",
    "\n",
    "local_minima_stats_by_redshift = sample_local_minima(data_by_redshift, num_samples=2000, sample_size=2000)\n",
    "param_grid_10 = {\n",
    "    'C': [0.005, 0.001, 0.0005],  \n",
    "    'gamma': [0.1, 0.5, 1],  \n",
    "    'kernel': ['linear', 'rbf', 'poly'] \n",
    "}\n",
    "# Below training code takes too long\n",
    "sample_local_minima_100_gs_results = svm_tune_hyperparam(param_grid_10, 100, local_minima_stats_by_redshift, 6)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
