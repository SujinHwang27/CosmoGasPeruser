2. Get access key and secret access key of the S3 bucket
Navigate to IAM -> Users 

3. add large dataset
uv run dvc add data/
# let git track dvc pointers 
git add data.dvc .gitignore
git commit -m "Track data with DVC"

4. Configure remote storage
dvc remote add -d myremote s3://your-bucket-name/ml-data
dvc remote modify myremote access_key_id <YOUR_ACCESS_KEY>
dvc remote modify myremote secret_access_key <YOUR_SECRET_KEY>

5. Push data to remote
dvc push

6. Pull data 
export AWS_ACCESS_KEY_ID="YOUR_ACTUAL_ACCESS_KEY_ID"
export AWS_SECRET_ACCESS_KEY="YOUR_ACTUAL_SECRET_ACCESS_KEY"
dvc pull

7. Track pipelines & experiments 
# define data processing pipelines in dvc.yaml
dvc run -n preprocess_dct \
    -d src/preprocess.py -d data/raw/input.csv \
    -o data/processed/dct_full \
    python src/preprocess.py
# This automatically tracks dependencies, outputs, and commands in DVC.

8. Split Responsibilities of DVC and MLflow
DVC (Data Version Control)	Primary Data Storage & Versioning (e.g., data/raw, data/processed). DVC stores these large files in a dedicated cloud/remote storage (S3, GCS).
MLflow	Experiment Artifacts (e.g., Trained Model Weights, Evaluation Plots, Metrics, Hyperparameters, Model Signatures). MLflow stores these in its artifact store (often configured to a separate S3 or GCS bucket).

1. Log DVC Checksums/Versions: Use the training script to read the DVC file for the data (e.g., data/processed.dvc) and log the data's unique checksum (or hash) as an MLflow parameter.
# In training script
data_hash = get_dvc_hash('data/processed.dvc')
mlflow.log_param("training_data_version", data_hash)

2. Log Git Commit Hash: Always log the Git commit hash to ensure full reproducibility of the code and data configuration.
# In training script
git_commit = get_git_commit()
mlflow.log_param("git_commit_id", git_commit)